{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import trafilatura\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import chardet\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(url_id,url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    # Extract the text from the HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    title = soup.find('meta', property='og:title').get('content')\n",
    "    text = trafilatura.extract(downloaded)\n",
    "    print(title,text)\n",
    "    f = open(f\"Data Extraction/{url_id}.txt\", \"a\")\n",
    "    f.write(title + \"\\n\")\n",
    "    if text is not None:\n",
    "        f.write(text)\n",
    "    else:\n",
    "        f.write('Text not present')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_links = pd.read_excel('Input.xlsx',engine='openpyxl', dtype=str)\n",
    "for index, row in input_links.iterrows():\n",
    "    data_extraction(row[\"URL_ID\"],row['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list to store stopwords\n",
    "stopwords_cust = []\n",
    "\n",
    "# Specify the folder containing text files with stopwords\n",
    "stopwords_folder_path = \"StopWords\"\n",
    "\n",
    "# Iterate through text files in the stopwords folder\n",
    "for filename in os.listdir(stopwords_folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(stopwords_folder_path, filename)\n",
    "        \n",
    "        # Detect the file's encoding\n",
    "        with open(file_path, 'rb') as file:\n",
    "            result = chardet.detect(file.read())\n",
    "\n",
    "        # Use the detected encoding to read the file\n",
    "        encoding = result['encoding']\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            file_contents = file.read()\n",
    "            stopwords_cust += file_contents.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postive_words = []\n",
    "postive_words_with_stopwords = []\n",
    "with open('MasterDictionary//positive-words.txt', 'r') as file:\n",
    "    file_contents = file.read()\n",
    "    postive_words_with_stopwords += file_contents.splitlines()\n",
    "    for i in range(len(postive_words_with_stopwords)):\n",
    "        if postive_words_with_stopwords[i] not in stopwords_cust:\n",
    "            postive_words.append(postive_words_with_stopwords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = []\n",
    "negative_words_with_stopwords = []\n",
    "with open('MasterDictionary//negative-words.txt', 'r') as file:\n",
    "    file_contents = file.read()\n",
    "    negative_words_with_stopwords += file_contents.splitlines()\n",
    "    for i in range(len(negative_words_with_stopwords)):\n",
    "        if negative_words_with_stopwords[i] not in stopwords_cust:\n",
    "            negative_words.append(negative_words_with_stopwords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    # Convert the word to lowercase for consistent comparison\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Define a list of vowels\n",
    "    vowels = \"aeiou\"\n",
    "    \n",
    "    # Initialize a count for syllables\n",
    "    syllable_count = 0\n",
    "    \n",
    "    # Check if the word ends with \"es\" or \"ed\" and adjust syllable count accordingly\n",
    "    if word.endswith(\"es\"):\n",
    "        syllable_count -= 1\n",
    "    elif word.endswith(\"ed\"):\n",
    "        syllable_count -= 1\n",
    "    \n",
    "    # Count the syllables by iterating through the characters in the word\n",
    "    for i in range(len(word)):\n",
    "        # Check if the character is a vowel and the next character is not a vowel to avoid double-counting\n",
    "        if word[i] in vowels and (i == len(word) - 1 or word[i + 1] not in vowels):\n",
    "            syllable_count += 1\n",
    "\n",
    "    # Adjust the count for single-letter words\n",
    "    if len(word) == 1 and word != \"a\" and word != \"i\":\n",
    "        syllable_count = 0\n",
    "\n",
    "    return syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder containing text files with stopwords\n",
    "data_folder_path = \"Data Extraction\"\n",
    "\n",
    "# Iterate through text files in the stopwords folder\n",
    "for filename in os.listdir(data_folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Define an empty list to store words\n",
    "        words = []\n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "        file_path = os.path.join(data_folder_path, filename)\n",
    "        \n",
    "        # Detect the file's encoding\n",
    "        with open(file_path, 'rb') as file:\n",
    "            result = chardet.detect(file.read())\n",
    "\n",
    "        # Use the detected encoding to read the file\n",
    "        encoding = result['encoding']\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            file_contents = file.read()\n",
    "            article_words = word_tokenize(file_contents)\n",
    "            article_sentences = sent_tokenize(file_contents)\n",
    "            for i in range(len(article_words)):\n",
    "                if article_words[i] not in stopwords_cust:\n",
    "                    words.append(article_words[i])\n",
    "        for i in words:\n",
    "            if i in postive_words:\n",
    "                positive_score += 1\n",
    "            elif i in negative_words:\n",
    "                negative_score -= 1\n",
    "        polarity_score = (positive_score + negative_score) / ((positive_score - negative_score) + 0.000001)\n",
    "        subjectivity_score = (positive_score - negative_score) / ((len(words) + 0.000001))\n",
    "        avg_wrd_sen = len(article_words) / len(article_sentences)\n",
    "        num_cmplx = 0\n",
    "        for i in words:\n",
    "            if count_syllables(i) >= 2:\n",
    "                   num_cmplx += 1\n",
    "        syll = 0\n",
    "        for i in words:\n",
    "            syll += count_syllables(i)\n",
    "        syll_per_word = syll / len(words)\n",
    "        per_cmplx_words = num_cmplx / len(words) \n",
    "        fog_indx = 0.4 * ( avg_wrd_sen + per_cmplx_words )\n",
    "        stop_words_nltk = set(stopwords.words('english'))\n",
    "        filtered_words = [w for w in article_words if not w.lower() in stop_words_nltk]\n",
    "        # initializing punctuations string\n",
    "        punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "        # Removing punctuations in string\n",
    "        # Using loop + punctuation string \n",
    "        for i in range(len(filtered_words)):\n",
    "            for ele in filtered_words[i]:\n",
    "                if ele in punc:\n",
    "                    filtered_words[i] = ele.replace(ele, \"\")\n",
    "        filtered_words = [w for w in filtered_words if w != \"\"]\n",
    "        word_count = len(filtered_words)\n",
    "        total_characters = sum(len(word) for word in filtered_words)\n",
    "        avg_wrd_len = total_characters /  word_count\n",
    "        # Define the personal pronouns you want to count\n",
    "        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        # Create a regex pattern to match the personal pronouns, but exclude \"US\"\n",
    "        pattern = r'\\b(?:' + '|'.join(personal_pronouns) + r')\\b(?!S\\b)'\n",
    "        # Use re.findall to find all matches of the pattern in the text\n",
    "        matches = re.findall(pattern, file_contents, flags=re.IGNORECASE)\n",
    "        # Count the number of matches\n",
    "        personal_pronoun_count = len(matches)\n",
    "        dataframe = pd.read_excel('Output Data Structure.xlsx',engine='openpyxl', dtype=str)\n",
    "        for index, row in dataframe.iterrows():\n",
    "            row_url_id = str(row['URL_ID']+'.txt')\n",
    "            if row_url_id == filename:\n",
    "                dataframe.loc[index, 'POSITIVE SCORE'] = positive_score\n",
    "                dataframe.loc[index, 'NEGATIVE SCORE'] = negative_score\n",
    "                dataframe.loc[index, 'POLARITY SCORE'] = polarity_score\n",
    "                dataframe.loc[index, 'SUBJECTIVITY SCORE'] = subjectivity_score\n",
    "                dataframe.loc[index, 'AVG SENTENCE LENGTH'] = avg_wrd_sen\n",
    "                dataframe.loc[index, 'PERCENTAGE OF COMPLEX WORDS'] = per_cmplx_words\n",
    "                dataframe.loc[index, 'FOG INDEX'] = fog_indx\n",
    "                dataframe.loc[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = avg_wrd_sen\n",
    "                dataframe.loc[index, 'COMPLEX WORD COUNT'] = num_cmplx\n",
    "                dataframe.loc[index, 'WORD COUNT'] = word_count\n",
    "                dataframe.loc[index, 'SYLLABLE PER WORD'] = syll_per_word\n",
    "                dataframe.loc[index, 'PERSONAL PRONOUNS'] = personal_pronoun_count\n",
    "                dataframe.loc[index, 'AVG WORD LENGTH'] = avg_wrd_len\n",
    "        dataframe.to_excel('Output Data Structure.xlsx', engine='openpyxl', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
